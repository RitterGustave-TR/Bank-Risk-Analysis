{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layer Perceptron Neural Network Classifier\n",
    "\n",
    "The MLP classifier is a Neural Network algorithm that is capable of building complex models, efficiently used on large datasets (such as ours). The fact that efficiency is important for this large dataset, a MLP was thought of a good selection. This method of machine learning is sensitive to scaling so keep in mind that we will be scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first, let's load our features in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# MUTATE DATAFRAMES ACCORDING TO THE EXPLORATORY DATA ANALYSIS CODE\n",
    "\n",
    "#For data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#In order to show all columns available\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "#Sklearn imports\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "\n",
    "#Graphing libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "apptrain = pd.read_csv('../Dataset/application_train.csv')\n",
    "apptest = pd.read_csv('../Dataset/application_test.csv')\n",
    "\n",
    "# Code that modifies dataframes\n",
    "apptrain['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "apptest['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "apptrain['DAYS_BIRTH'] = abs(apptrain['DAYS_BIRTH'])\n",
    "apptest['DAYS_BIRTH'] = abs(apptrain['DAYS_BIRTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (307511, 243)\n",
      "Training features shape: (48744, 239)\n",
      "3 columns were label encoded\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding and dataframe alignment\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through columns\n",
    "for col in apptrain:\n",
    "    if apptrain[col].dtype == \"object\":\n",
    "        if len(list(apptrain[col].unique())) <= 2:\n",
    "            #train on the training data\n",
    "            le.fit(apptrain[col])\n",
    "            #transform both training and testing data\n",
    "            apptrain[col] = le.transform(apptrain[col])\n",
    "            apptest[col] = le.transform(apptest[col])\n",
    "            \n",
    "            le_count += 1\n",
    "            \n",
    "\n",
    "            \n",
    "#One-Hot encoding\n",
    "apptrain = pd.get_dummies(apptrain)\n",
    "apptest = pd.get_dummies(apptest)\n",
    "\n",
    "\n",
    "\n",
    "print('Training features shape: {}'.format(apptrain.shape))\n",
    "print('Training features shape: {}'.format(apptest.shape))\n",
    "print('{} columns were label encoded'.format(le_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 240)\n",
      "Testing features shape:  (48744, 239)\n",
      "We're back on track, remember the training dataset will have one column more since it DOES have the targets\n"
     ]
    }
   ],
   "source": [
    "# Take the labels out of the training dataset as an inner merge will erase them since the test dataset does not have the targets\n",
    "train_labels = apptrain['TARGET']\n",
    "\n",
    "\n",
    "#aligning the training and testing data, keep only columns present in both df's\n",
    "apptrain, apptest = apptrain.align(apptest, join = 'inner', axis = 1)\n",
    "apptrain['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape: ', apptrain.shape)\n",
    "print('Testing features shape: ', apptest.shape)\n",
    "print(\"We're back on track, remember the training dataset will have one column more since it DOES have the targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape (307511, 239)\n",
      "testing data shape (48744, 239)\n"
     ]
    }
   ],
   "source": [
    "# Scaling not very much required for Random Forest models\n",
    "# Creating base df's for machine learning model\n",
    "training_data = apptrain.drop(columns = ['TARGET'])\n",
    "testing_data = apptest.copy()\n",
    "\n",
    "# In the dataframes we still have missing values, WE USE IMPUTATION HERE\n",
    "imputer = Imputer(strategy = 'median')\n",
    "imputer.fit(training_data)\n",
    "imputer.fit(testing_data)\n",
    "training_data = imputer.transform(training_data)\n",
    "testing_data = imputer.transform(testing_data)\n",
    "\n",
    "# We must scale the data as well.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "training_data = scaler.fit_transform(training_data)\n",
    "testing_data = scaler.transform(testing_data)\n",
    "\n",
    "\n",
    "print('training data shape', training_data.shape)\n",
    "print('testing data shape', testing_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Neural Network (BASE) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=800, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(activation='relu', learning_rate='adaptive', max_iter=800)\n",
    "mlp.fit(training_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rGust\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "mlp_base_predictions = mlp.predict_proba(testing_data)[:, 1]\n",
    "\n",
    "def format_and_submit(predictions, desired_file_name):\n",
    "    submit = apptest[['SK_ID_CURR']]\n",
    "    submit['TARGET'] = predictions\n",
    "    submit.to_csv('../Model_Predictions/{}.csv'.format(desired_file_name), index = False)\n",
    "    \n",
    "\n",
    "format_and_submit(mlp_base_predictions, 'MLP_NN_Base_Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dataframe with the most useful features\n",
    "fe_training_data = pd.read_csv('rf_important_features.csv')\n",
    "fe_testing_data = pd.read_csv('rftest_important_features.csv')\n",
    "\n",
    "train_labels =  fe_training_data['TARGET']\n",
    "fe_training_data = fe_training_data.drop(columns = 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, MinMaxScaler\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "fe_training_data = imputer.fit_transform(fe_training_data)\n",
    "fe_testing_data = imputer.transform(fe_testing_data)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "scaler.fit(fe_training_data)\n",
    "fe_training_data = scaler.transform(fe_training_data)\n",
    "fe_testing_data = scaler.transform(fe_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25502566\n",
      "Iteration 2, loss = 0.25167207\n",
      "Iteration 3, loss = 0.25083626\n",
      "Iteration 4, loss = 0.25001302\n",
      "Iteration 5, loss = 0.24979902\n",
      "Iteration 6, loss = 0.24950077\n",
      "Iteration 7, loss = 0.24905607\n",
      "Iteration 8, loss = 0.24872079\n",
      "Iteration 9, loss = 0.24837681\n",
      "Iteration 10, loss = 0.24813010\n",
      "Iteration 11, loss = 0.24794531\n",
      "Iteration 12, loss = 0.24757039\n",
      "Iteration 13, loss = 0.24732049\n",
      "Iteration 14, loss = 0.24700897\n",
      "Iteration 15, loss = 0.24670407\n",
      "Iteration 16, loss = 0.24668559\n",
      "Iteration 17, loss = 0.24635933\n",
      "Iteration 18, loss = 0.24620464\n",
      "Iteration 19, loss = 0.24581054\n",
      "Iteration 20, loss = 0.24570083\n",
      "Iteration 21, loss = 0.24546659\n",
      "Iteration 22, loss = 0.24521398\n",
      "Iteration 23, loss = 0.24522515\n",
      "Iteration 24, loss = 0.24484288\n",
      "Iteration 25, loss = 0.24474555\n",
      "Iteration 26, loss = 0.24455575\n",
      "Iteration 27, loss = 0.24434301\n",
      "Iteration 28, loss = 0.24431514\n",
      "Iteration 29, loss = 0.24404097\n",
      "Iteration 30, loss = 0.24392111\n",
      "Iteration 31, loss = 0.24388113\n",
      "Iteration 32, loss = 0.24373209\n",
      "Iteration 33, loss = 0.24346985\n",
      "Iteration 34, loss = 0.24340528\n",
      "Iteration 35, loss = 0.24328093\n",
      "Iteration 36, loss = 0.24316497\n",
      "Iteration 37, loss = 0.24307941\n",
      "Iteration 38, loss = 0.24287173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(activation='relu', learning_rate='adaptive', max_iter=1000, verbose=True)\n",
    "mlp.fit(fe_training_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
